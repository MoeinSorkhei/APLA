## APLA: A Simple Adaptation Method for Vision Transformers

APLA (Attention Projection Layer Adaptation) is a lightweight yet effective method for adapting Vision Transformers (ViTs).

We identify the projection layer immediately following the attention mechanism as critical for adaptation. 
APLA tunes only this layer—or even a _random_ subset of its weights—without modifying the architecture or adding parameters.

Despite its simplicity, APLA delivers exceptional performance with remarkable efficiency.


<div align="center" style="margin-bottom: 1px;">
  <img src="figs/fig_1_mem.png" width="48%" style="margin-right: 0.2%;">
  <img src="figs/fig_1_latency.png" width="48%">
</div>
<div align="center">
  <img src="figs/ViT-components.png" width="97%">
</div>


### Usage
Code is coming soon!

